{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQoW0445gTc9",
        "outputId": "7816ed0d-d9d1-428b-98e8-d8288506402c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-16 13:47:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-09-16 13:47:05 (27.3 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Dataset Preparation**\n",
        "We will use Shakespearean text data as the corpus for training the model.\n",
        "\n",
        "**Code to load and preprocess the data:**"
      ],
      "metadata": {
        "id": "6xGvb4a0kz26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load dataset (Shakespeare corpus)\n",
        "with open(\"shakespeare.txt\", 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create a character-to-index and index-to-character mapping\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Convert the text into integer indices\n",
        "text_as_int = np.array([char_to_idx[c] for c in text])\n",
        "\n",
        "# Set sequence length for training\n",
        "SEQ_LENGTH = 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Create input-output sequences\n",
        "def create_sequences(text, seq_length):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(text) - seq_length):\n",
        "        inputs.append(text[i:i+seq_length])\n",
        "        targets.append(text[i+seq_length])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "inputs, targets = create_sequences(text_as_int, SEQ_LENGTH)\n",
        "\n",
        "# Create a PyTorch dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "dataset = TextDataset(inputs, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "tOs0J2j1kZ4H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Model Definition (Transformer)**\n",
        "We'll define a Transformer model for text generation.\n",
        "\n",
        "**Code to define the Transformer-based text generation model:**"
      ],
      "metadata": {
        "id": "jbnuLC_Ok8wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = nn.Embedding(SEQ_LENGTH, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, num_heads, num_layers, num_layers, dropout=dropout)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Add positional encoding to the input\n",
        "        seq_length = src.shape[1]\n",
        "        pos = torch.arange(0, seq_length).unsqueeze(0).repeat(src.size(0), 1).to(src.device)\n",
        "        embedded = self.embedding(src) + self.pos_encoder(pos)\n",
        "\n",
        "        # Transformer expects (sequence_length, batch_size, embedding_dim)\n",
        "        embedded = embedded.transpose(0, 1)\n",
        "        transformer_output = self.transformer(embedded, embedded)\n",
        "        output = self.fc(transformer_output[-1])  # Take the output of the last token\n",
        "        return output"
      ],
      "metadata": {
        "id": "GVkW-pZUlERo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Training Loop**\n",
        "We'll now define the training loop to train the model on the dataset.\n",
        "\n",
        "**Training code:**"
      ],
      "metadata": {
        "id": "C-SrtA8-lSbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "VOCAB_SIZE = len(chars)\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 4\n",
        "EPOCHS = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerModel(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            loss = loss_fn(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Train the model\n",
        "train(model, dataloader, optimizer, loss_fn, EPOCHS)\n",
        "\n",
        "# Save model checkpoints\n",
        "torch.save(model.state_dict(), \"transformer_text_gen.pth\")"
      ],
      "metadata": {
        "id": "8BPeNuu1lb3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2d7fa1-0f06-43fd-8606-55fcbc5c805b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.3200660777829385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Text Generation**\n",
        "To generate text, we'll use the trained model to predict the next character, using a softmax function to sample the most likely next character.\n",
        "\n",
        "**Text generation code:**"
      ],
      "metadata": {
        "id": "SDKQfsWIle2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, gen_length):\n",
        "    model.eval()\n",
        "    input_eval = [char_to_idx[s] for s in start_text]\n",
        "    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_text = start_text\n",
        "    for _ in range(gen_length):\n",
        "        with torch.no_grad():\n",
        "            output = model(input_eval)\n",
        "            prediction = torch.softmax(output, dim=-1)\n",
        "            next_char_idx = torch.multinomial(prediction, num_samples=1).item()\n",
        "            next_char = idx_to_char[next_char_idx]\n",
        "            generated_text += next_char\n",
        "\n",
        "            # Prepare the input for the next step\n",
        "            input_eval = torch.cat([input_eval[:, 1:], torch.tensor([[next_char_idx]], dtype=torch.long).to(device)], dim=1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate a text sample\n",
        "start_text = \"To be, or not to be, \"\n",
        "generated = generate_text(model, start_text, 500)\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "apaXPJW9lkfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445f8af4-8f6a-433a-dd31-594f3e441afe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be, or not to be, dla\n",
            "IhfS\n",
            "le cAh h.:soth lhm!u IhbheesnsarysoVempwfCrei gt,np neipitS\n",
            "vse Vetil gatpvrYy tmaua  etYctytonR srte \n",
            "kuar eb\n",
            "ttwa o uoH hrw,fDtcwlr \n",
            "tr, cumie  Fo  ethnuteshekteb oRt :s\n",
            " tnucw\n",
            "nnleeh,Waeymd, eHrnpI meta asa Elne zLCAt . t aukheWafl eb Rouydielot,Npe i,hcnib.Les\n",
            "\n",
            "oueheM,,nwo   f,taacunttFslto   \n",
            "hoyy i\n",
            ",eA  rcohslh,l hfrydn tdsuaeehdcscwieehog dwirosmarroedeasl u wO iike,ieb!M nDe'L'fe Heyaasmmus nBoIft gth asy\n",
            "b sr \n",
            "oesshm eeet, PsUscuo\n",
            " ? a;tl:v,gietsn et iode a ra\n",
            "teuadsbI \n",
            "hri\n",
            "  s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the efficiancy increases when the epoch is increased but it needs large compute power"
      ],
      "metadata": {
        "id": "NQAJgCjeNvmA"
      }
    }
  ]
}